# 為替トレードバックテスト検証ツール

（7/4 現時点ではgeminiにAIを作ってもらった段階です。見た感じ正しくない内容がちょこちょこあります。7月中旬頃にはさらに詳しく正確なREADMEを公開できるようにします）
（また、2GBでは収まらないcsvデータが多く含まれており、LFSでもpush失敗となったのでデータ類はリポジトリにありません。csvを証券会社からダウンロードして、pklファイル変換が必要です）

## 概要

このプロジェクトは、外国為替（FX）市場における特定の取引戦略の有効性を検証するためのバックテストツール、および将来的なリアルタイム取引への拡張を目的とした自動売買システムです。

主な特徴として、単純なゴールデンクロスやデッドクロスのような一般的なシグナルに依存せず、複数のテクニカル指標（移動平均線、ピボット、フィボナッチリトレースメント）とセッシ���ン情報を組み合わせた、より複合的な条件でエントリーポイントを判断するロジックを検証しています。

また、大量の過去データを用いた高速なバックテストを実現するため、データ形式の工夫（Pickle化）、並列処理による計算の高速化、NumPy/Pandasライブラリによるベクトル演算など、パフォーマンスを重視した設計がなされています。

## 主な特徴

-   **複合的なエントリーロジック**: 複数のテクニカル指標を組み合わせた、独自の押し目買い・戻り売り戦略を検証します。
-   **高速バックテスト**: NumPyによるベクトル演算、並列処理、データキャッシングにより、大量のデータでも高速に検証できます。
-   **セッション分析**: 東京、ロンドン、ニューヨークといった主要市場のセッション時間に基づいた分析機能を持ちます。
-   **柔軟なデータ管理**: CSVデータから高速なPickle形式への変換機能を備えています。
-   **詳細なログ出力**: トレードごとの詳細なログ��CSVファイルに出力し、詳細な分析が可能です。

## 取引戦略について

このツールで検証しているのは、単純なトレンドフォロー戦略ではありません。主なアイデアは以下の通りです。

**「短期的な相場の過熱感や調整局面を狙った、押し目買い・戻り売り戦略」**

具体的なエントリー条件は、以下のような複数の要素を組み合わせて判断されます。

1.  **トレンドの方向性**: 中長期の移動平均線（例: 75MA）を用いて、全体的なトレンドの方向を把握します。
2.  **エントリーのタイミング**:
    -   短期の移動平均線（例: 20MA）とローソク足の位置関係から、短期的な調整局面（押し目・戻り）を判断します。
    -   これは、**ゴールデンクロスを待つのではなく、トレンド方向に価格が一時的に調整したタイミングを狙う**という工夫点です。
3.  **サポート＆レジスタンス**:
    -   **ピボット (Pivot Points)**: 前日の価格から計算されるサポートラインとレジスタンスライン。
    -   **フィボナッチ・リトレースメント (Fibonacci Retracement)**: 直近の高値・安値から計算される押し目・戻りの目安。
    -   これらのライン付近での反発をエントリーの根拠として加えます。
4.  **セッション情報**: 特定の市場（例: ロンドン市場）が開いている時間帯はボラティリティが高まる傾向があるため、セッション情報をエントリー条件の一部として考慮します。

これらの条件を複合的に組み合わせることで、より精度の高いエントリーポイントを探ることを目的としています。

## セットアップと実行方法

### 1. 前提条件

-   Python 3.8以上
-   必要なライブラリ: `pandas`, `numpy`など。（`requirements.txt`がないため、必要に応じてインストールしてください）

### 2. リポジトリのクローン

```bash
git clone https://github.com/neoaamu2000/forex_old.git
cd forex_old
```

### 3. データ（Pickleファイル）の準備

**重要**: このリポジトリには、バックテストを高速に実行するための`pickle`形式のデータファイルは含まれていません。初回実行前、またはCSVデータを更新した際には、必ずデータ変換処理を実行する必要があります。

`test_file/currency_data/`内に配置されている`USDJPY_data.csv`などのソースデータから、`manage_data.py`などのデータ管理スクリプトを用いて`pickle`ファイルを生成します。csv読み込みのまま読み込む形式にしても良いですが、読み込みだけで数十秒要することになるため、pklを一度生成することをお勧めします。

```bash
# manage_data.pyやそれに類するスクリプトでデータ変換を実行する例
python manage_data.py --convert-to-pickle
```

これにより、`test_file/pickle_data/`ディレクトリ内に、処理済みのデータが保存され、バックテスト時に高速に読み込めるようになります。

## 処理フロー（バックテスト）

このプロジェクトの根幹をなすバックテスト処理は `test_file/main.py` の `process_data` 関数からキックオフされます。リアルタイム処理とは異なり、指定された期間のデータを一度に処理し、取引戦略の有効性を検証します。処理は主にNumPy配列を用いて高速に行われます。

1.  **初期設定とデータ読み込み**
    -   `process_data(conditions)`: バックテストの各種パラメータ（通貨ペア、期間、SMAの期間、各種閾値など）を辞書形式で受け取ります。
    -   `pd.read_pickle()`: `pickle_data` ディレクトリから指定された通貨ペアの過去データ（例: `USDJPY_1M.pkl`）をPandas DataFrameとして高速に読み込みます。
    -   読み込んだデータをNumPy配列 `np_arr` に変換し、以降の計算は主にこの配列上で行われます。

2.  **テクニカル指標の並列計算準備 (`pre_data_process`)**
    -   基準となる**20期間SMA (BASE_SMA)**と、短期的な値動きを見る**4期間SMA (SML_SMA)**の計算準備をそれぞれ行います。
    -   `calculate_sma()`: 各SMAを計算します。
    -   `determine_trend()`: SMAの傾きから、短期的なトレンドの方向（上昇/下降）を判定し、配列に追加します。

3.  **ピボット���波の頂点と底）の検出 (`detect_pivots`)**
    -   `pre_data_process`から呼び出され、20MAと4MAのそれぞれに対して、トレンドの転換点となる重要な高値（トップ）と安値（ボトム）を検出します。
    -   このピボットは、後続の「波」を定義するための起点となります。

4.  **波（セッション）の生成 (`WaveManager.add_session`)**
    -   20MAのピボットが検出されるたびに、`WaveManager`が新しい「波」のインスタンス（`MyModel`クラス）を生成します。
    -   `add_session()`: 前回のピボットから今回のピボットまでを一つの「波」として定義します。例えば、安値ピボット→高値ピボットであれば「上昇波」、高値ピボット→安値ピボットであれば「下降波」として管理されます。

5.  **各波の分析とエントリー判定 (`WaveManager.analyze_sessions`)**
    -   `analyze_sessions()`: 生成された全ての波に対して、一括で分析処理を実行します。
    -   `MyModel.execute_state_action()`: 各波のインスタンスが、自身の状態（押し目を待っている状態、ポジション保有中など）に応じて、エントリーやエグジットの判断を行います。
        -   **フィボナッチ・リトレースメント**: 波の始点と終点（ピボット）からフィボナッチ・リトレースメントを計算します。
        -   **エントリー条件**: 価格が指定されたフィボナッチレベル（例: 38.2%）に達し、かつ他の条件（例: 短期MAの反転）を満たした場合にエントリーを決定します。
        -   **損切り・利確**: エントリーと同時に、損切りライン（例: 直近の短期ピボット）と利確ライン（例: フィボナッチ・エクステンションの138.2%）を計算します。
        -   時間経過と共に価格が損切り・利確ラインに達したかを判定し、トレード結果を記録します。

6.  **結果の集計と出力 (`WaveManager.summarize_and_export_results`)**
    -   全ての波の分析が完了した後、`trade_logs`に記録された全トレード結果を集計します。
    -   `summarize_and_export_results()`: 以下の統計情���を計算し、最終的なトレードログと共に出力先のCSVファイルに追記します。
        -   プロフィットファクター
        -   平均リスクリワード比率
        -   最大連敗数
        -   最大ドローダウン（動的リスク管理シミュレーションに基づく）
        -   勝率
        -   資金推移グラフの元データ

このフローにより、特定の取引戦略とパラメータ設定が、過去のデータに対してどのようなパフォーマンスを示したかを高速かつ詳細に検証することができます。

## パフォーマンス向上のための工夫

このプロジェクトでは、高速な検証サイクルを実現するために、以下の技術的な工夫が施されています。

### 1. NumPy/Pandasによるベクトル演算

-   金融データのような大規模な時系列データを扱う際、Python標準の`for`ループで各行を処理すると非常に低速になります。
-   本プロジェクトでは、`NumPy`および`Pandas`ライブラリを全面的に活用しています。これにより、移動平均の��算や条件判定などを「ベクトル演算」として一括で処理できます。
-   ベクトル演算は、内部的にC言語などの高速な言語で実装されているため、ループ処理に比べて数十倍から数百倍高速に動作します。

### 2. 並列処理による計算高速化

-   バックテスト、特にパラメータの最適化を行う際には、膨大な量の計算が必要です。
-   `test_file/main_parallels.py`からわかるように、`multiprocessing`などのライブラリを利用して、計算処理を複数のCPUコアに分散させています。
-   これにより、異なる通貨ペアや異なるパラメータ設定のバックテストを同時に実行でき、全体の検証時間を劇的に短縮します。

### 3. データキャッシング（Pickle形式の利用）

-   数十MBから数百MBになることもあるCSVファイルは、テキスト形式であるため、プログラム実行のたびに解析（パース）に時間がかかります。
-   `pickle`は、Pythonオブジェクトをバイナリ形式でシリアライズ（保存）するための仕組��です。
-   一度`DataFrame`オブジェクトとして読み込んだデータを`pickle`ファイルとして保存しておくことで、次回以降はテキストの解析が不要になり、データの読み込み時間を大幅に短縮できます。

## ディレクトリ構成

-   `リアルタイム本番用/`: 実際の取引口座と連携して動作させるための本番用コードが格納されています。
-   `test_file/`: バックテストに関連するファイル群。データ、テスト用スクリプト、検証結果などが含まれます。
-   `セッション開発用ファイル/`: 市場セッションに関連する機能の開発・検証用ファイルが格納されています。
-   `アーカイブ/`: 過去のバージョンのコードや、実験的に作成したスクリプトが保管されています。
-   `矢印表示用/`: `matplotlib`などを利用して、チャート上に売買シグナルを矢印で表示するためのスクリプトです。
